---
title: "Analysis 2 - Simulation-estimation"
format: html
editor: visual
---

# Simulation-estimation studies to infer and account for activity levels

What is the scale of the problem and to what extent can we overcome it using our proposed model?

Demonstrate underestimation of R0 when activity level not accounted for

\- Illustration that individual variation in contact rates increases the eigenvalue. This effect is stronger with higher assortativity, but still the case with less assortativity

see Leon's analysis in ContactMatrixSplit.qmd for mostly complete code

NEED: single-panel figure: number of activity classes on bottom, lines for each assortativity level.

Demonstrate we can estimate activity levels

\- Illustration with toy example (based on Tom's) that we can estimate the variance with a random effects model, and learn the parameters

See Nick's analysis in tom_contact_demo.qmd for a full example on a simple simulation

NEED: rework tom_contact_demo.qmd to have slightly more realistic data simulation from a known model, fit the model, and estimate the true parameters

Demonstrate we can build augmented contact matrices and recover a less-biased R0

\- Using the estimated parameters, build a kroneckered multi-activity level contact matrix and recover a less-biased R0. See how this depends on the number of classes

NEED: rework tom_contact_demo.qmd to have multiple activity classes and demonstrate that it approaches true eigenvalues

(is 'true' eigenvalue just the limiting case where all individuals have their own class in the matrix?)

Load packages and functions

```{r}
#| message: false
#| warning: false
source("R/packages.R")
source("R/functions.R")
```

## Demonstrate scale of problem

Demonstrate underestimation of R0 when activity level not accounted for

Illustration that individual variation in contact rates increases the eigenvalue. This effect is stronger with higher assortativity, but still the case with less assortativity.

Run computations in parallel

```{r}
plan(multisession, workers = 8)
```

### Old plot

Apply for a range of assortativities and numbers of activity level bins

```{r}
output <- expand_grid(
  assort = seq(0, 1, 0.1),
  bins = seq(1, 10, 1)
)|> 
  mutate(
    tmp = furrr::future_map2(assort,
                             bins,
                             ~ map_to_eigen(assort = .x,
                                            n_classes = .y),
                             .options = furrr_options(seed = TRUE))
  ) |> 
  unnest(
    tmp
  )
```

Plot these
```{r}
output |> 
  select(
    assort,
    bins,
    eigenvalue = tmp
  ) |> 
  mutate(
    bins = as_factor(bins)
  )|> 
  ggplot(
    aes(
      x = assort,
      y = eigenvalue,
      colour = bins)
  ) +
  geom_line() +
  geom_point() +
  xlab("Assortativity")
```

### New plot

In the above plot, assortativity is on the x axis and therefore appears as the primary point of the plot, whereas the manuscript is more about activity levels.

Also, the colour is plotted as the number of bins, which is a metric of quality of the approximation than a measure of variation in activity levels.
Now, try a new plot that plots eigenvalues against the activity level standard deviation sigma (x axis) with assortativity in colour, with number of bins fixed at a high level.

```{r}
output <- expand_grid(
  sigma = seq(0, 1, 0.05),
  assort = seq(0, 1, 0.2)
)|> 
  mutate(
    eigenvalue = furrr::future_map2(sigma,
                                    assort,
                                    ~ map_to_eigen(sigma = .x,
                                                   assort = .y,
                                                   n_classes = 300),
                             .options = furrr_options(seed = TRUE))
  ) |> 
  unnest(
    eigenvalue
  )
```

Plot the results

```{r}
activity_assort_plot <- output |> 
  select(
    Assortativity = assort,
    sigma,
    eigenvalue
  ) |> 
  ggplot(
    aes(
      x = sigma,
      y = eigenvalue,
      group = Assortativity,
      colour = Assortativity)
  ) +
  geom_line() +
  scale_color_gradient(
    low = grey(0.8),
    high = "blue"
  ) +
  xlab("Variation in activity levels") +
  ylab("Eigenvalue") +
  theme_minimal()
activity_assort_plot
```

Save this plot for the manuscript

```{r}
ggsave("figures/activity_assort_plot.png",
       plot = activity_assort_plot,
       bg = "white",
       scale = 1,
       width = 6,
       height = 4)
```

## Tom's concern simulation-estimation analysis

Define a finite number of individuals and observations:

```{r}
n_individuals <- 1000
n_observations <- 100
observations <- expand_grid(
  individual = seq_len(n_individuals),
  observation = seq_len(n_observations)
)
```

Now create a separate data object for each of the cases above. For Case 1, individuals either have all 3s or all 6s:

```{r}
case_1 <- observations %>%
  group_by(
    individual
  ) %>%
  mutate(
    contacts = sample(x = c(3, 6),
                      size = 1,
                      replace = TRUE)
  ) %>%
  ungroup()

# print the first 3 observations per individual to check
case_1 %>%
  filter(
    observation < 4
  ) %>%
  print(
    n = 15
  )
```

For Case 2, all individuals are the same, and each observation is either 3 or 6:

```{r}
case_2 <- observations %>%
  mutate(
    contacts = sample(x = c(3, 6),
                      size = n(),
                      replace = TRUE)
  )

# print the first 3 observations per individual to check
case_2 %>%
  filter(
    observation < 4
  ) %>%
  print(
    n = 15
  )
```

The vectors of contacts for the two cases have the same distribution (mean = 4.5, sd = 1.5)

```{r}
mean(case_1$contacts)
sd(case_1$contacts)
mean(case_2$contacts)
sd(case_2$contacts)
```

## Models

Now we estimate the between-individual variance for these two cases using a random effects model.

For case 1, statistical estimation is hampered by the fact that the observation variance in the data is 0. This leads to errors in the fitting algorithm and an incorrect estimate. For this reason, we need to add some additional variance to each observation to get the model to fit. This won't be an issue with real data.

With some jitter, the model can estimate the between-individual variance correctly, and the additional observation variance (which we later discard).

```{r}
case_1_jitter <- case_1 %>%
  mutate(
    contacts = contacts + rnorm(n(), 0, 0.05)
  )
case_1_model <- lmer(contacts ~ (1|individual),
                     data = case_1_jitter)
summary(case_1_model)$varcor
```

For case 2 (which has plenty of observation variance), the same model fits well on the degenerate data and correctly estimates that the variation (sd \~\~ 1.5) is mostly within individuals. The between-individual sd is constrained in the statistical model to be greater than 0, but occasionally singular fits occur and we get a warning and an estimate of 0 (the truth, for case 2).

```{r}
case_2_model <- lmer(contacts ~ (1|individual),
                     data = case_2)
summary(case_2_model)$varcor
```

We can check the model estimates of the individual-level means for the two different scenarios/datasets. It correctly identifies that in the Case 1 data, individuals are (approximately) from two different classes (mean 3 and mean 6), but in Case 2 they are (approximately) all from the same class with mean 4.5.

```{r}
estimated_social_activities <- tibble(
  individual = seq_len(n_individuals)
) %>%
  mutate(
    mean_case_1 = predict(case_1_model,
                          newdata = .),
    mean_case_2 = predict(case_2_model,
                          newdata = .)
  )
estimated_social_activities
```

## Eigenvalues

Now we construct two-stage (high and low social activity) contact matrices from these and compute the dominant eigenvalues.

### From the observed sample of individuals

First we do this using the estimated mean individual contact rates for the *observed sample* of individuals. This is slightly different than the proposed analysis for real data, which will simulate individuals from the modelled population distribution (a normal distribution, which is a poor match to these data but which we also do below for comparison). This version should give the answer most relevant to Tom's question.

```{r}
# define a small number, to make the code work with the boundary issues for case 2
epsilon <- sqrt(.Machine$double.eps)
classes_sample <- estimated_social_activities %>%
  pivot_longer(
    cols = starts_with("mean_"),
    names_to = "case",
    values_to = "mean_contacts",
    names_prefix = "mean_"
  ) %>%
  mutate(
    # add jitter to the mean contacts, to deal with singular (variance = 0) fits
    # in case 2
    mean_contacts_jitter = mean_contacts + rnorm(n(), 0, epsilon),
  ) %>%
  group_by(
    case
  ) %>%
  mutate(
    class = case_when(
      mean_contacts_jitter > median(mean_contacts) ~ "high",
      mean_contacts_jitter <= median(mean_contacts) ~ "low"
    )
  ) %>%
  group_by(
    case,
    class
  ) %>%
  summarise(
    mean = mean(mean_contacts),
    .groups = "drop"
  ) %>%
  pivot_wider(
    values_from = mean,
    names_from = case
  )
classes_sample
```

Now we can create contact matrices and compute dominant eigenvalues, which match the expected values.

For case 1, the eigenvalue is (approximately) 5, as expected.

```{r}
case_1_sample_contact <- make_contact_matrix(classes_sample$case_1)
case_1_sample_contact
get_eigenval(case_1_sample_contact)
```

For case 2, the eigenvalue is (approximately) the average number of contacts: 4.5, as expected.

```{r}
case_2_sample_contact <- make_contact_matrix(classes_sample$case_2)
case_2_sample_contact
get_eigenval(case_2_sample_contact)
```

### From the modelled population

For comparison, we also run the analysis as envisaged for real data. This uses the normal model for population variability (which will be more powerful where we have fewer respondents and repeat observations). In this degenerate data case, the model's normality assumption is violated (in case 1), so the estimates of contact rates for the two classes, and the dominant eigenvalue, are biased down but still clearly greater than for case 2.

```{r}
case_1_mean <- fixef(case_1_model)
case_1_sd <- attr(summary(case_1_model)$varcor$individual, "stddev")
case_2_mean <- fixef(case_2_model)
case_2_sd <- attr(summary(case_2_model)$varcor$individual, "stddev")

classes_population <- tibble(
  case_1 = rnorm(1e5, case_1_mean, case_1_sd),
  case_2 = rnorm(1e5, case_2_mean, case_2_sd)
) %>%
  pivot_longer(
    cols = everything(),
    names_to = "case",
    values_to = "mean_contacts"
  ) %>%
  mutate(
    # add jitter to the mean contacts, to deal with singular (variance = 0) fits
    # in case 2
    mean_contacts_jitter = mean_contacts + rnorm(n(), 0, epsilon),
  ) %>%
  group_by(
    case
  ) %>%
  mutate(
    class = case_when(
      mean_contacts_jitter > median(mean_contacts) ~ "high",
      mean_contacts_jitter <= median(mean_contacts) ~ "low"
    )
  ) %>%
  group_by(
    case,
    class
  ) %>%
  summarise(
    mean = mean(mean_contacts),
    .groups = "drop"
  ) %>%
  pivot_wider(
    values_from = mean,
    names_from = case
  )
classes_population
case_1_population_contact <- make_contact_matrix(classes_population$case_1)
case_1_population_contact
get_eigenval(case_1_population_contact)
case_2_population_contact <- make_contact_matrix(classes_population$case_2)
case_2_population_contact
get_eigenval(case_2_population_contact)
```

In an analysis on a real dataset (with less data and without this two-pointmass data structure), this second version is likely to provide less biased estimates than the first.
